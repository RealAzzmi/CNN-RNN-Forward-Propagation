Experiment: lstm_layers_2
==================================================

Configuration:
  vocab_size: 2743
  embedding_dim: 128
  lstm_units: 64
  num_lstm_layers: 2
  bidirectional: False
  dropout_rate: 0.3
  dense_units: 32
  num_classes: 3
  max_sequence_length: 128

Validation Results:
  Macro F1: 0.7381
  Accuracy: 0.7400

Test Results:
  Macro F1: 0.7524
  Accuracy: 0.7600

Classification Report:
{'negative': {'precision': 0.6949152542372882, 'recall': 0.803921568627451, 'f1-score': 0.7454545454545455, 'support': 153.0}, 'neutral': {'precision': 0.7325581395348837, 'recall': 0.65625, 'f1-score': 0.6923076923076923, 'support': 96.0}, 'positive': {'precision': 0.8613138686131386, 'recall': 0.7814569536423841, 'f1-score': 0.8194444444444444, 'support': 151.0}, 'accuracy': 0.76, 'macro avg': {'precision': 0.7629290874617701, 'recall': 0.7472095074232783, 'f1-score': 0.7524022274022274, 'support': 400.0}, 'weighted avg': {'precision': 0.7667650236355946, 'recall': 0.76, 'f1-score': 0.7606304875679877, 'support': 400.0}}