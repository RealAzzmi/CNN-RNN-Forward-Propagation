Experiment: lstm_units_128
==================================================

Configuration:
  vocab_size: 2743
  embedding_dim: 128
  lstm_units: 128
  num_lstm_layers: 1
  bidirectional: False
  dropout_rate: 0.3
  dense_units: 32
  num_classes: 3
  max_sequence_length: 128

Validation Results:
  Macro F1: 0.7820
  Accuracy: 0.7900

Test Results:
  Macro F1: 0.7270
  Accuracy: 0.7375

Classification Report:
{'negative': {'precision': 0.7483443708609272, 'recall': 0.738562091503268, 'f1-score': 0.743421052631579, 'support': 153.0}, 'neutral': {'precision': 0.6428571428571429, 'recall': 0.65625, 'f1-score': 0.6494845360824743, 'support': 96.0}, 'positive': {'precision': 0.7880794701986755, 'recall': 0.7880794701986755, 'f1-score': 0.7880794701986755, 'support': 151.0}, 'accuracy': 0.7375, 'macro avg': {'precision': 0.7264269946389152, 'recall': 0.7276305205673145, 'f1-score': 0.7269950196375762, 'support': 400.0}, 'weighted avg': {'precision': 0.738027436140019, 'recall': 0.7375, 'f1-score': 0.7377348412913727, 'support': 400.0}}