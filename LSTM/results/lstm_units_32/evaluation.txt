Experiment: lstm_units_32
==================================================

Configuration:
  vocab_size: 2743
  embedding_dim: 128
  lstm_units: 32
  num_lstm_layers: 1
  bidirectional: False
  dropout_rate: 0.3
  dense_units: 32
  num_classes: 3
  max_sequence_length: 128

Validation Results:
  Macro F1: 0.7600
  Accuracy: 0.7700

Test Results:
  Macro F1: 0.7449
  Accuracy: 0.7575

Classification Report:
{'negative': {'precision': 0.7058823529411765, 'recall': 0.7843137254901961, 'f1-score': 0.7430340557275542, 'support': 153.0}, 'neutral': {'precision': 0.7435897435897436, 'recall': 0.6041666666666666, 'f1-score': 0.6666666666666666, 'support': 96.0}, 'positive': {'precision': 0.8223684210526315, 'recall': 0.8278145695364238, 'f1-score': 0.8250825082508251, 'support': 151.0}, 'accuracy': 0.7575, 'macro avg': {'precision': 0.7572801725278505, 'recall': 0.7387649872310954, 'f1-score': 0.7449277435483487, 'support': 400.0}, 'weighted avg': {'precision': 0.7589056174089069, 'recall': 0.7575, 'f1-score': 0.755679173180476, 'support': 400.0}}